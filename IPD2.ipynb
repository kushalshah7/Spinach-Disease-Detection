{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLq8AVemTkLmCMvdGJAhmQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalshah7/Spinach-Disease-Detection/blob/main/IPD2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "JaK9vTBCS4Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbNPJjxjYSNt",
        "outputId": "f73613c2-8550-4fec-e34a-34bcfe32af9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# base_dir = '/content/drive/MyDrive/Colab Notebooks/Dataset'\n",
        "# dataset_dir = os.path.join(base_dir, 'Dataset')\n",
        "# gan_train_dir = os.path.join(base_dir, 'GAN_Train')\n",
        "# gan_output_dir = os.path.join(base_dir, 'GAN_Images')\n",
        "\n",
        "# os.makedirs(gan_train_dir, exist_ok=True)\n",
        "# os.makedirs(gan_output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "1mwjznMwYVC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "z_dim = 100\n",
        "image_size = 64\n",
        "batch_size = 64\n",
        "num_epochs = 20  # Change as needed\n",
        "learning_rate = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n"
      ],
      "metadata": {
        "id": "eE9vSuuBXwLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(z_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64), nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n"
      ],
      "metadata": {
        "id": "1v0hIZCvx-OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_gan_on_class(class_name, data_path, save_dir, num_generate=500):\n",
        "    dataset = ImageFolder(root=data_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    netG = Generator().to(device)\n",
        "    netD = Discriminator().to(device)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizerD = torch.optim.Adam(netD.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "    optimizerG = torch.optim.Adam(netG.parameters(), lr=learning_rate, betas=(beta1, 0.999))\n",
        "\n",
        "    real_label = 1.\n",
        "    fake_label = 0.\n",
        "\n",
        "    print(f\"Training GAN for class: {class_name}\")\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (real_images, _) in enumerate(dataloader):\n",
        "            b_size = real_images.size(0)\n",
        "            real_images = real_images.to(device)\n",
        "            label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            netD.zero_grad()\n",
        "            output = netD(real_images)\n",
        "            errD_real = criterion(output, label)\n",
        "            errD_real.backward()\n",
        "\n",
        "            noise = torch.randn(b_size, z_dim, 1, 1, device=device)\n",
        "            fake = netG(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = netD(fake.detach())\n",
        "            errD_fake = criterion(output, label)\n",
        "            errD_fake.backward()\n",
        "            optimizerD.step()\n",
        "\n",
        "            # Train Generator\n",
        "            netG.zero_grad()\n",
        "            label.fill_(real_label)\n",
        "            output = netD(fake)\n",
        "            errG = criterion(output, label)\n",
        "            errG.backward()\n",
        "            optimizerG.step()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] Loss_D: {errD_real+errD_fake:.4f}, Loss_G: {errG:.4f}\")\n",
        "\n",
        "        # Show 5 samples at every 20th epoch\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            netG.eval()\n",
        "            with torch.no_grad():\n",
        "                z = torch.randn(5, z_dim, 1, 1, device=device)\n",
        "                samples = netG(z).cpu()\n",
        "                grid = torchvision.utils.make_grid(samples, nrow=5, normalize=True)\n",
        "                plt.figure(figsize=(10, 2))\n",
        "                plt.imshow(grid.permute(1, 2, 0))\n",
        "                plt.axis(\"off\")\n",
        "                plt.title(f\"{class_name} - Generated samples at epoch {epoch+1}\")\n",
        "                plt.show()\n",
        "            netG.train()\n",
        "\n",
        "    # Save generated images\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    netG.eval()\n",
        "    for i in range(num_generate):\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(1, z_dim, 1, 1, device=device)\n",
        "            generated = netG(z)\n",
        "            save_image(generated, f\"{save_dir}/{class_name}_gan_{i}.png\", normalize=True)\n"
      ],
      "metadata": {
        "id": "xpChLxuDyAHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def merge_generated_images(generated_dir, main_dataset_dir):\n",
        "    for class_name in os.listdir(generated_dir):\n",
        "        gen_class_path = os.path.join(generated_dir, class_name)\n",
        "        main_class_path = os.path.join(main_dataset_dir, class_name)\n",
        "\n",
        "        os.makedirs(main_class_path, exist_ok=True)\n",
        "        for img_file in os.listdir(gen_class_path):\n",
        "            src = os.path.join(gen_class_path, img_file)\n",
        "            dst = os.path.join(main_class_path, img_file)\n",
        "            shutil.copy(src, dst)\n",
        "    print(\"Merged generated images into main dataset.\")\n"
      ],
      "metadata": {
        "id": "sP_JOEPIyLoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.folder import default_loader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Custom dataset to load images directly from flat class folders\n",
        "class SimpleImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.file_paths = [\n",
        "            os.path.join(root_dir, fname) for fname in os.listdir(root_dir)\n",
        "            if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "        ]\n",
        "        self.transform = transform\n",
        "        self.loader = default_loader\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.loader(self.file_paths[idx])\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, 0  # Dummy label\n",
        "\n",
        "\n",
        "# Replace ImageFolder with SimpleImageDataset in train_gan_on_class\n",
        "def train_gan_on_class(class_name, data_path, save_dir, num_generate=500):\n",
        "    dataset = SimpleImageDataset(data_path, transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # ... (rest of your GAN training code remains unchanged)\n"
      ],
      "metadata": {
        "id": "sOtLRYR_zotN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_path = \"/content/drive/My Drive/Colab Notebooks/Dataset\"\n",
        "generated_path = \"/content/drive/My Drive/Colab Notebooks/Generated\"\n",
        "\n",
        "# Ensure generated path exists\n",
        "os.makedirs(generated_path, exist_ok=True)\n",
        "\n",
        "for class_name in os.listdir(base_path):\n",
        "    class_dir = os.path.join(base_path, class_name)\n",
        "    if not os.path.isdir(class_dir):\n",
        "        continue  # Skip .DS_Store or other files\n",
        "\n",
        "    # Skip folders like \"GAN_Train\" or \"GAN_Images\"\n",
        "    if class_name.startswith(\"GAN\"):\n",
        "        print(f\"Skipping {class_name} — likely output folders.\")\n",
        "        continue\n",
        "\n",
        "    # Check for valid images\n",
        "    valid_imgs = [\n",
        "        f for f in os.listdir(class_dir)\n",
        "        if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
        "    ]\n",
        "    if len(valid_imgs) == 0:\n",
        "        print(f\"Skipping {class_name} — no valid images found.\")\n",
        "        continue\n",
        "\n",
        "    gen_dir = os.path.join(generated_path, class_name)\n",
        "    train_gan_on_class(class_name, class_dir, gen_dir, num_generate=500)\n",
        "\n",
        "# Only merge if generated_path has subfolders with images\n",
        "if os.path.exists(generated_path) and any(os.scandir(generated_path)):\n",
        "    merge_generated_images(generated_path, base_path)\n",
        "else:\n",
        "    print(\"No generated images found to merge.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kScjuGPgyDpL",
        "outputId": "99db5732-abf3-41b4-959f-a3a94d2ce5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping GAN_Train — likely output folders.\n",
            "Skipping GAN_Images — likely output folders.\n",
            "No generated images found to merge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_kDrVPbcyHrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# num_generate = 500\n",
        "# z = torch.randn(num_generate, latent_size, 1, 1).to(device)\n",
        "# gen_images = G(z)\n",
        "# for i, img in enumerate(gen_images):\n",
        "#     save_image(img, f'{gan_output_dir}/gan_img_{i}.png', normalize=True)\n"
      ],
      "metadata": {
        "id": "bxpK_9gAX0ZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # import shutil\n",
        "# # import os\n",
        "\n",
        "# # for img_file in os.listdir(gan_output_dir):\n",
        "# #     shutil.copy(os.path.join(gan_output_dir, img_file), os.path.join(dataset_dir, 'Anthracnose'))\n",
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# src_dir = gan_output_dir\n",
        "# dst_dir = os.path.join(dataset_dir, 'Anthracnose')  # make sure this exists\n",
        "\n",
        "# os.makedirs(dst_dir, exist_ok=True)  # ensure target folder exists\n",
        "\n",
        "# for img_file in os.listdir(src_dir):\n",
        "#     full_src_path = os.path.join(src_dir, img_file)\n",
        "#     full_dst_path = os.path.join(dst_dir, img_file)\n",
        "\n",
        "#     if os.path.isfile(full_src_path):  # skip any subdirs, just in case\n",
        "#         shutil.copy(full_src_path, full_dst_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "U4Qbj0HaX1Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpinachDiseaseModel(nn.Module):\n",
        "    def __init__(self, num_classes=5):\n",
        "        super(SpinachDiseaseModel, self).__init__()\n",
        "        self.model = models.resnet50(pretrained=True)\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False  # Freeze all layers initially\n",
        "\n",
        "        num_ftrs = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(num_ftrs, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "1m4wUIKyS6dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    model.to(device)\n",
        "    train_acc_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        correct, total, running_loss = 0, 0, 0.0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_accuracy = 100 * correct / total\n",
        "        train_acc_list.append(train_accuracy)\n",
        "\n",
        "        val_accuracy = evaluate(model, val_loader, device)\n",
        "        val_acc_list.append(val_accuracy)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}: Loss = {running_loss:.4f}, Train Acc = {train_accuracy:.2f}%, Val Acc = {val_accuracy:.2f}%\")\n",
        "\n",
        "    return train_acc_list, val_acc_list\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return 100 * correct / total\n"
      ],
      "metadata": {
        "id": "Vfjl3tcrS8hO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],  # ImageNet mean\n",
        "                         [0.229, 0.224, 0.225])  # ImageNet std\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(root=\"dataset/train\", transform=transform)\n",
        "val_dataset = ImageFolder(root=\"dataset/val\", transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "vLx_4Fr7S-3R",
        "outputId": "1af668f4-19d1-4a8a-805c-0dfeac5e5fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-e3471f844222>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset/train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset/val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SpinachDiseaseModel(num_classes=5)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.model.fc.parameters(), lr=1e-4)\n",
        "\n",
        "train_acc, val_acc = train(model, train_loader, val_loader, criterion, optimizer, epochs=10, device=device)\n"
      ],
      "metadata": {
        "id": "FBj03MOFTAnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lime.lime_image import LimeImageExplainer\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Preprocess the image for LIME\n",
        "def preprocess_image_lime(img_path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img = Image.open(img_path)\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    return img\n",
        "\n",
        "# Function to predict using the model (LIME needs this)\n",
        "def predict_fn(images):\n",
        "    images = torch.tensor(images).float()\n",
        "    outputs = model(images)\n",
        "    return F.softmax(outputs, dim=1).detach().numpy()\n",
        "\n",
        "# LIME explanation\n",
        "def explain_with_lime(image_path):\n",
        "    img = preprocess_image_lime(image_path)\n",
        "    img_numpy = img.squeeze().numpy().transpose(1, 2, 0)  # Convert to HWC format\n",
        "\n",
        "    explainer = LimeImageExplainer()\n",
        "    explanation = explainer.explain_instance(img_numpy, predict_fn, top_labels=5, hide_color=0, num_samples=1000)\n",
        "\n",
        "    # Get the explanation for the top predicted class\n",
        "    temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
        "    plt.imshow(mark_boundaries(temp, mask))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "explain_with_lime('path_to_image.jpg')\n"
      ],
      "metadata": {
        "id": "BC6coip1XnPq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}